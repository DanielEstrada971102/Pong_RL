{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto de RL -Primera implementación\n",
    "### Entorno tipo Atari personalizado - PongGame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.0 (SDL 2.0.16, Python 3.8.10)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# descomente el entorno que quiere importar\n",
    "#from PongGame_Env import *\n",
    "from PongGame_Env2 import *\n",
    "\n",
    "import os\n",
    "from stable_baselines3 import A2C, DQN\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from numpy import mean, std, sqrt\n",
    "\n",
    "env_v = \"2\" # cambie por 1 si usa el primer entorno o 2 en caso contrario\n",
    "algorithm = \"DQN\" # Escriba \"A2C\" o \"DQN\" segun el algoritmo que usara\n",
    "training_steps = 80000\n",
    "\n",
    "# ponga el numero de pasos para evaluar\n",
    "n_episodes_check = 20\n",
    "\n",
    "# ruta para guardar los logs del entrenamiento y el modelo\n",
    "log_path = os.path.join(\"Training\", \"Logs\")\n",
    "algorithm_path = os.path.join('Training','Saved_Models', algorithm + '_Pongenv' + env_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se crea el entorno con el cual el agente iteractuara\n",
    "env = PongGame(game_speed = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN selected\n"
     ]
    }
   ],
   "source": [
    "# se define el agente con un modelo (policy) de Mlp : multi-layer Perceptron \n",
    "if algorithm == 'A2C':\n",
    "    print(\"A2C selected\")\n",
    "    model = A2C(\"MlpPolicy\", env, tensorboard_log=log_path, verbose=0, seed=123)\n",
    "elif algorithm == 'DQN':\n",
    "    print(\"DQN selected\")\n",
    "    model = DQN(\"MlpPolicy\", env, tensorboard_log=log_path, verbose=0, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, cum_reward 539,  Score 0\n",
      "Episode 1, cum_reward 172,  Score -1\n",
      "Episode 2, cum_reward 145,  Score -1\n",
      "Episode 3, cum_reward 238,  Score -1\n",
      "Episode 4, cum_reward 236,  Score -1\n",
      "Episode 5, cum_reward 236,  Score -1\n",
      "Episode 6, cum_reward 217,  Score -1\n",
      "Episode 7, cum_reward 473,  Score 0\n",
      "Episode 8, cum_reward 836,  Score 1\n",
      "Episode 9, cum_reward 502,  Score 0\n",
      "Episode 10, cum_reward 836,  Score 1\n",
      "Episode 11, cum_reward 217,  Score -1\n",
      "Episode 12, cum_reward 145,  Score -1\n",
      "Episode 13, cum_reward 236,  Score -1\n",
      "Episode 14, cum_reward 172,  Score -1\n",
      "Episode 15, cum_reward 172,  Score -1\n",
      "Episode 16, cum_reward 236,  Score -1\n",
      "Episode 17, cum_reward 473,  Score 0\n",
      "Episode 18, cum_reward 836,  Score 1\n",
      "Episode 19, cum_reward 820,  Score 1\n",
      "mean_reward : 386.85 +- 57.82051516273622\n"
     ]
    }
   ],
   "source": [
    "# Se chequa el desempeño del agente antes de ser entrenado\n",
    "# si corre este bloque de código se recomienda reiniciar el entorno de ejecucion\n",
    "# al final, y no ejecutarlo para hacer el entrenamiento.\n",
    "\n",
    "rewards = []\n",
    "for episode in range(n_episodes_check):\n",
    "    obs = env.reset()\n",
    "    done = 0\n",
    "    score = 0\n",
    "    c_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs) # accione aleatoria\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        env.render()\n",
    "        c_reward += reward\n",
    "        score = env.CURRENT_GAME_SCORE\n",
    "    \n",
    "    print(\"Episode %d, cum_reward %d,  Score %d\"%(episode,c_reward, score))\n",
    "    rewards.append(c_reward)\n",
    "\n",
    "print(\"mean_reward : {} +- {}\".format(mean(rewards), std(rewards, ddof=1)/ sqrt(20)))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se define un callback para obtener información del entrenamiento\n",
    "eval_callback = EvalCallback(env, eval_freq=1000, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-15 23:10:24.664101: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64:\n",
      "2021-12-15 23:10:24.664434: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x7f344ae29ac0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se ejecuta el entrenamiento del agente...\n",
    "model.learn(total_timesteps=training_steps, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se guarda el modelo...\n",
    "model.save(algorithm_path)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN selected\n"
     ]
    }
   ],
   "source": [
    "# se carga el modelo entrenado\n",
    "if algorithm == 'A2C':\n",
    "    print(\"A2C selected\")\n",
    "    model = A2C.load(algorithm_path)\n",
    "elif algorithm == 'DQN':\n",
    "    print(\"DQN selected\")\n",
    "    model = DQN.load(algorithm_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, cum_reward 544.00,  Score 0\n",
      "Episode 1, cum_reward 230.00,  Score -1\n",
      "Episode 2, cum_reward 194.00,  Score -1\n",
      "Episode 3, cum_reward 200.00,  Score -1\n",
      "Episode 4, cum_reward 535.00,  Score 0\n",
      "Episode 5, cum_reward 218.00,  Score -1\n",
      "Episode 6, cum_reward 192.00,  Score -1\n",
      "Episode 7, cum_reward 229.00,  Score -1\n",
      "Episode 8, cum_reward 228.00,  Score -1\n",
      "Episode 9, cum_reward 548.00,  Score 0\n",
      "Episode 10, cum_reward 236.00,  Score -1\n",
      "Episode 11, cum_reward 222.00,  Score -1\n",
      "Episode 12, cum_reward 550.00,  Score 0\n",
      "Episode 13, cum_reward 550.00,  Score 0\n",
      "Episode 14, cum_reward 230.00,  Score -1\n",
      "Episode 15, cum_reward 201.00,  Score -1\n",
      "Episode 16, cum_reward 230.00,  Score -1\n",
      "Episode 17, cum_reward 533.00,  Score 0\n",
      "Episode 18, cum_reward 543.00,  Score 0\n",
      "Episode 19, cum_reward 181.00,  Score -1\n",
      "mean_reward : 329.7 +- 36.115756876145376\n"
     ]
    }
   ],
   "source": [
    "# Se chequa el desempeño del agente luego de ser entrenado\n",
    "\n",
    "rewards = []\n",
    "for episode in range(n_episodes_check):\n",
    "    obs = env.reset()\n",
    "    done = 0\n",
    "    score = 0\n",
    "    c_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs) # accione aleatoria\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        env.render()\n",
    "        c_reward += reward\n",
    "        score = env.CURRENT_GAME_SCORE\n",
    "    \n",
    "    print(\"Episode %d, cum_reward %.2f,  Score %d\"%(episode,c_reward, score))\n",
    "    rewards.append(c_reward)\n",
    "\n",
    "print(\"mean_reward : {} +- {}\".format(mean(rewards), std(rewards, ddof=1)/ sqrt(20)))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 0: cd: {log_path}: No such file or directory\n",
      "2021-12-15 23:29:57.832279: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-15 23:29:57.832353: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2021-12-15 23:30:00.478898: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-12-15 23:30:00.478967: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-12-15 23:30:00.478994: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (DESKTOP-VFD3AH7): /proc/driver/nvidia/version does not exist\n",
      "W1215 23:30:05.507452 139621211211584 server_ingester.py:187] Failed to communicate with data server at localhost:45233: <_InactiveRpcError of RPC that terminated with:\n",
      "\tstatus = StatusCode.DEADLINE_EXCEEDED\n",
      "\tdetails = \"Deadline Exceeded\"\n",
      "\tdebug_error_string = \"{\"created\":\"@1639629005.505340500\",\"description\":\"Deadline Exceeded\",\"file\":\"src/core/ext/filters/deadline/deadline_filter.cc\",\"file_line\":81,\"grpc_status\":4}\"\n",
      ">\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.7.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n",
      "terminate called without an active exception\n",
      "Fatal Python error: Aborted\n",
      "\n",
      "Thread 0x00007efc13b0e700 (most recent call first):\n",
      "  File \"/usr/lib/python3.8/socket.py\", line 669 in readinto\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/werkzeug/serving.py\", line 370 in handle_one_request\n",
      "  File \"/usr/lib/python3.8/http/server.py\", line 429 in handle\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/werkzeug/serving.py\", line 342 in handle\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 747 in __init__\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 360 in finish_request\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 683 in process_request_thread\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 870 in run\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 932 in _bootstrap_inner\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 890 in _bootstrap\n",
      "\n",
      "Thread 0x00007efbc571c700 (most recent call first):\n",
      "  File \"/usr/lib/python3.8/socket.py\", line 669 in readinto\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/werkzeug/serving.py\", line 370 in handle_one_request\n",
      "  File \"/usr/lib/python3.8/http/server.py\", line 429 in handle\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/werkzeug/serving.py\", line 342 in handle\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 747 in __init__\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 360 in finish_request\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 683 in process_request_thread\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 870 in run\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 932 in _bootstrap_inner\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 890 in _bootstrap\n",
      "\n",
      "Thread 0x00007efc10b0c700 (most recent call first):\n",
      "  File \"/usr/lib/python3.8/socket.py\", line 669 in readinto\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/werkzeug/serving.py\", line 370 in handle_one_request\n",
      "  File \"/usr/lib/python3.8/http/server.py\", line 429 in handle\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/werkzeug/serving.py\", line 342 in handle\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 747 in __init__\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 360 in finish_request\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 683 in process_request_thread\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 870 in run\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 932 in _bootstrap_inner\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 890 in _bootstrap\n",
      "\n",
      "Current thread 0x00007efc1330d700 (most recent call first):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/lib/io/file_io.py\", line 705 in is_directory_v2\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/lib/io/file_io.py\", line 691 in is_directory\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/lib/io/file_io.py\", line 878 in walk_v2\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/lib/io/file_io.py\", line 889 in walk_v2\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/lib/io/file_io.py\", line 889 in walk_v2\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/lib/io/file_io.py\", line 889 in walk_v2\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/tensorboard/backend/event_processing/io_wrapper.py\", line 172 in ListRecursivelyViaWalking\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/tensorboard/backend/event_processing/io_wrapper.py\", line 220 in <genexpr>\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/tensorboard/backend/event_processing/plugin_event_multiplexer.py\", line 198 in AddRunsFromDirectory\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/tensorboard/backend/event_processing/data_ingester.py\", line 98 in _reload\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 870 in run\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 932 in _bootstrap_inner\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 890 in _bootstrap\n",
      "\n",
      "Thread 0x00007efc18b28740 (most recent call first):\n",
      "<no Python frame>\n"
     ]
    }
   ],
   "source": [
    "# para ver graficamente la informaciónd entrenamiento se debe correr\n",
    "!cd {log_path}\n",
    "!tensorboard --logdir=."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "df0893f56f349688326838aaeea0de204df53a132722cbd565e54b24a8fec5f6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
